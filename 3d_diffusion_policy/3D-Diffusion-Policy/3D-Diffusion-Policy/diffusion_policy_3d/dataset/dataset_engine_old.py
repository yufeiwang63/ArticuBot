from collections import defaultdict, Counter
import itertools
import math
from pathlib import Path
import random
from time import time

import torch
from torch.utils.data import Dataset

from .utils import loader, Resize, Rotate, TrajectoryInterpolator

# chialiang
import zarr, glob, os, copy
import numpy as np


class RLBenchDataset(Dataset):
    """RLBench dataset."""

    def __init__(
        self,
        # required
        # root,
        zarr_path,
        instructions=None,
        # dataset specification
        taskvar=[('close_door', 0)],
        max_episode_length=5,
        cache_size=0,
        max_episodes_per_task=100,
        num_iters=None,
        cameras=("wrist", "left_shoulder", "right_shoulder"),
        # for augmentations
        training=True,
        gripper_loc_bounds=None,
        image_rescale=(1.0, 1.0),
        point_cloud_rotate_yaw_range=0.0,
        # for trajectories
        return_low_lvl_trajectory=False,
        dense_interpolation=False,
        interpolation_length=100,
        action_dim=8,  # elements of action trajectory to regress
        predict_short=None,  # for short-term open-loop
        # for chialiang
        val_ratio=0.1,
        train_ratio=0.9,
        **kwargs
    ):
        self._cache = {}
        self._cache_size = cache_size
        self._cameras = cameras
        self._max_episode_length = max_episode_length
        self._num_iters = num_iters
        self._training = training
        self._taskvar = taskvar
        self._return_low_lvl_trajectory = return_low_lvl_trajectory
        self._action_dim = action_dim
        if isinstance(root, (Path, str)):
            root = [Path(root)]
        # self._root = [Path(r).expanduser() for r in root]
        self._predict_short = predict_short

        # for chialiang
        self.load_per_step = kwargs.get('load_per_step', False)

        # For trajectory optimization, initialize interpolation tools
        if return_low_lvl_trajectory:
            assert dense_interpolation or self._predict_short
            self._interpolate_traj = TrajectoryInterpolator(
                use=dense_interpolation,
                interpolation_length=interpolation_length
            )

        # # We keep only useful instructions to save mem
        # self._instructions = defaultdict(dict)
        # self._num_vars = Counter()  # variations of the same task
        # for root, (task, var) in itertools.product(self._root, taskvar):
        #     data_dir = root / f"{task}+{var}"
        #     if data_dir.is_dir():
        #         if instructions is not None:
        #             self._instructions[task][var] = instructions[task][var]
        #         self._num_vars[task] += 1

        # If training, initialize augmentation classes
        if self._training:
            self._resize = Resize(scales=image_rescale)
            self._rotate = Rotate(
                gripper_loc_bounds=gripper_loc_bounds,
                yaw_range=point_cloud_rotate_yaw_range
            )
            assert point_cloud_rotate_yaw_range == 0.

        # File-names of episodes per-task and variation
        self._data_dirs = []
        # episodes_by_task = defaultdict(list)
        # for root, (task, var) in itertools.product(self._root, taskvar):
        all_zarr_paths = copy.deepcopy(zarr_path)

        all_paths = []
        train_masks = []
        val_masks = []
        for zarr_path in all_zarr_paths:

            all_subfolder = os.listdir(zarr_path)
            # import pdb; pdb.set_trace()
            for string in ["action_dist", "demo_rgbs", "all_demo_path.txt", "meta_info.json", 'example_pointcloud', '.zgroup']:
                if string in all_subfolder:
                    all_subfolder.remove(string)
            all_subfolder = sorted(all_subfolder)
            n_episodes = len(all_subfolder)
            num_load_episodes = kwargs.get('num_load_episodes', n_episodes)
            num_load_episodes = min(num_load_episodes, n_episodes)
            all_subfolder = all_subfolder[:num_load_episodes]
            zarr_paths = [os.path.join(zarr_path, subfolder) for subfolder in all_subfolder]
            all_paths += zarr_paths
            folder_train_mask = np.zeros(num_load_episodes, dtype=bool)
            folder_train_mask[:int(num_load_episodes*train_ratio)] = True
            train_masks.append(folder_train_mask)
            folder_val_mask = np.zeros(num_load_episodes, dtype=bool)
            folder_val_mask[-int(num_load_episodes*val_ratio):] = True
            val_masks.append(folder_val_mask)
            
        self.val_mask = np.zeros(self.replay_buffer.n_episodes, dtype=bool)
        self.val_mask[-int(self.replay_buffer.n_episodes*val_ratio):] = True
        train_mask = np.zeros(self.replay_buffer.n_episodes, dtype=bool)
        train_mask[:int(self.replay_buffer.n_episodes*train_ratio)] = True

            # episodes_by_task[task] += episodes

            # data_dir = root / f"{task}+{var}"
            # if not data_dir.is_dir():
            #     print(f"Can't find dataset folder {data_dir}")
            #     continue
            # npy_episodes = [(task, var, ep) for ep in data_dir.glob("*.npy")]
            # dat_episodes = [(task, var, ep) for ep in data_dir.glob("*.dat")]
            # pkl_episodes = [(task, var, ep) for ep in data_dir.glob("*.pkl")]
            # episodes = npy_episodes + dat_episodes + pkl_episodes
            # if max_episodes_per_task > -1:
            #     episodes = episodes[
            #         :max_episodes_per_task // self._num_vars[task] + 1
            #     ]
            # Split episodes equally into task variations
            # if len(episodes) == 0:
            #     print(f"Can't find episodes at folder {data_dir}")
            #     continue
            # self._data_dirs.append(data_dir)
            # episodes_by_task[task] += episodes

        # # Collect and trim all episodes in the dataset
        # self._episodes = []
        # self._num_episodes = 0
        # for task, eps in episodes_by_task.items():
        #     if len(eps) > max_episodes_per_task and max_episodes_per_task > -1:
        #         eps = random.sample(eps, max_episodes_per_task)
        #     self._episodes += eps
        #     self._num_episodes += len(eps)

        # print(f"Created dataset from {root} with {self._num_episodes}")
            
    def get_validation_dataset(self):
        val_set = copy.copy(self)
        val_set.train_mask = self.val_mask
        return val_set

    def read_from_cache(self, args):
        if self._cache_size == 0:
            return loader(args)

        if args in self._cache:
            return self._cache[args]

        value = loader(args)

        if len(self._cache) == self._cache_size:
            key = list(self._cache.keys())[int(time()) % self._cache_size]
            del self._cache[key]

        if len(self._cache) < self._cache_size:
            self._cache[args] = value

        return value

    @staticmethod
    def _unnormalize_rgb(rgb):
        # (from [-1, 1] to [0, 1]) to feed RGB to pre-trained backbone
        return rgb / 2 + 0.5

    def __getitem__(self, episode_id):
        """
        the episode item: [
            [frame_ids],  # we use chunk and max_episode_length to index it
            [obs_tensors],  # wrt frame_ids, (n_cam, 2, 3, 256, 256)
                obs_tensors[i][:, 0] is RGB, obs_tensors[i][:, 1] is XYZ
            [action_tensors],  # wrt frame_ids, (1, 8)
            [camera_dicts],
            [gripper_tensors],  # wrt frame_ids, (1, 8)
            [trajectories]  # wrt frame_ids, (N_i, 8)
        ]
        """
        episode_id %= self._num_episodes
        task, variation, file = self._episodes[episode_id]

        # Load episode
        episode = self.read_from_cache(file)
        if episode is None:
            return None

        # Dynamic chunking so as not to overload GPU memory
        chunk = random.randint(
            0, math.ceil(len(episode[0]) / self._max_episode_length) - 1
        )

        # Get frame ids for this chunk
        frame_ids = episode[0][
            chunk * self._max_episode_length:
            (chunk + 1) * self._max_episode_length
        ]

        # Get the image tensors for the frame ids we got
        states = torch.stack([
            episode[1][i] if isinstance(episode[1][i], torch.Tensor)
            else torch.from_numpy(episode[1][i])
            for i in frame_ids
        ])

        # Camera ids
        if episode[3]:
            cameras = list(episode[3][0].keys())
            assert all(c in cameras for c in self._cameras)
            index = torch.tensor([cameras.index(c) for c in self._cameras])
            # Re-map states based on camera ids
            states = states[:, index]

        # Split RGB and XYZ
        rgbs = states[:, :, 0]
        pcds = states[:, :, 1]
        rgbs = self._unnormalize_rgb(rgbs)

        # Get action tensors for respective frame ids
        action = torch.cat([episode[2][i] for i in frame_ids])

        # # Sample one instruction feature
        # if self._instructions:
        #     instr = random.choice(self._instructions[task][variation])
        #     instr = instr[None].repeat(len(rgbs), 1, 1)
        # else:
        #     instr = torch.zeros((rgbs.shape[0], 53, 512))

        # Get gripper tensors for respective frame ids
        gripper = torch.cat([episode[4][i] for i in frame_ids])

        # gripper history
        gripper_history = torch.stack([
            torch.cat([episode[4][max(0, i-2)] for i in frame_ids]),
            torch.cat([episode[4][max(0, i-1)] for i in frame_ids]),
            gripper
        ], dim=1)

        # Low-level trajectory
        traj, traj_lens = None, 0
        if self._return_low_lvl_trajectory:
            traj_items = [
                self._interpolate_traj(episode[5][i]) for i in frame_ids
            ]
            max_l = max(len(item) for item in traj_items)
            traj = torch.zeros(len(traj_items), max_l, 8)
            traj_lens = torch.as_tensor(
                [len(item) for item in traj_items]
            )
            for i, item in enumerate(traj_items):
                traj[i, :len(item)] = item
            traj_mask = torch.zeros(traj.shape[:-1])
            for i, len_ in enumerate(traj_lens.long()):
                traj_mask[i, len_:] = 1

        # Augmentations
        if self._training:
            pcds, gripper, action, traj = self._rotate(
                pcds, gripper, action, None, traj
            )
            if traj is not None:
                for t, tlen in enumerate(traj_lens):
                    traj[t, tlen:] = 0
            modals = self._resize(rgbs=rgbs, pcds=pcds)
            rgbs = modals["rgbs"]
            pcds = modals["pcds"]

        ret_dict = {
            "task": [task for _ in frame_ids],
            "rgbs": rgbs,  # e.g. tensor (n_frames, n_cam, 3+1, H, W)
            "pcds": pcds,  # e.g. tensor (n_frames, n_cam, 3, H, W)
            "action": action[..., :self._action_dim],  # e.g. tensor (n_frames, 8), target pose
            # "instr": instr,  # a (n_frames, 53, 512) tensor
            "curr_gripper": gripper[..., :self._action_dim],
            "curr_gripper_history": gripper_history[..., :self._action_dim]
        }
        if self._return_low_lvl_trajectory:
            ret_dict.update({
                "trajectory": traj[..., :self._action_dim],  # e.g. tensor (n_frames, T, 8)
                "trajectory_mask": traj_mask.bool()  # e.g. tensor (n_frames, T)
            })
        return ret_dict

    def __len__(self):
        if self._num_iters is not None:
            return self._num_iters
        return self._num_episodes